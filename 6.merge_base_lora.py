# -*- coding: utf-8 -*-
import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import sys

# --- è·¯å¾„é…ç½® ---
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.append(project_root)

BASE_MODEL_PATH = "./models/gemma-3-270m-it"
LORA_MODEL_PATH = "./checkpoints/lora_gemma_generation"
MERGED_MODEL_PATH = "./models/merged_gemma_lora"

# --- è®¾å¤‡é…ç½® ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def merge_lora_with_base_model():
    """
    åŠ è½½åŸºç¡€æ¨¡å‹å’ŒLoRAé€‚é…å™¨ï¼Œå°†å®ƒä»¬åˆå¹¶ï¼Œå¹¶ä¿å­˜åˆå¹¶åçš„æ¨¡å‹ã€‚
    """
    print("=" * 30)
    print("ğŸš€ å¼€å§‹èåˆ LoRA æ¨¡å‹ä¸åŸºç¡€æ¨¡å‹...")
    print("=" * 30)

    # 1. åŠ è½½åˆ†è¯å™¨
    print(f"ğŸ”„ æ­£åœ¨ä» '{BASE_MODEL_PATH}' åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("âœ… åˆ†è¯å™¨åŠ è½½å®Œæˆã€‚")

    # 2. åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"ğŸ”„ æ­£åœ¨ä» '{BASE_MODEL_PATH}' åŠ è½½åŸºç¡€æ¨¡å‹...")
    offload_folder = "./offload"
    os.makedirs(offload_folder, exist_ok=True)
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32,
        device_map='auto', # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        offload_folder=offload_folder
    )
    print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    # 3. åŠ è½½LoRAé€‚é…å™¨å¹¶ä¸åŸºç¡€æ¨¡å‹åˆå¹¶
    print(f"ğŸ”„ æ­£åœ¨ä» '{LORA_MODEL_PATH}' åŠ è½½LoRAé€‚é…å™¨å¹¶è¿›è¡Œåˆå¹¶...")
    # åŠ è½½PeftModel
    model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH, offload_folder=offload_folder)
    # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
    merged_model = model.merge_and_unload()
    print("âœ… LoRAæ¨¡å‹åˆå¹¶å®Œæˆã€‚")

    # 4. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹å’Œåˆ†è¯å™¨
    print(f"ğŸ’¾ æ­£åœ¨å°†åˆå¹¶åçš„æ¨¡å‹ä¿å­˜åˆ° '{MERGED_MODEL_PATH}'...")
    os.makedirs(MERGED_MODEL_PATH, exist_ok=True)

    print("ğŸ”„ æ­£åœ¨å°†æ¨¡å‹ç§»è‡³CPUä»¥ä¾¿ä¿å­˜ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´å’Œå†…å­˜)...")
    merged_model = merged_model.to("cpu")
    print("âœ… æ¨¡å‹å·²ç§»è‡³CPUã€‚")

    merged_model.save_pretrained(MERGED_MODEL_PATH)
    tokenizer.save_pretrained(MERGED_MODEL_PATH)

    # --- 5. å¤åˆ¶ tokenizer.model ä»¥ç¡®ä¿å…¼å®¹æ€§ ---
    print("ğŸ”„ æ­£åœ¨å¤åˆ¶ tokenizer.model æ–‡ä»¶ä»¥ç¡®ä¿GGUFè½¬æ¢å…¼å®¹æ€§...")
    try:
        import shutil
        source_tokenizer_model = os.path.join(BASE_MODEL_PATH, "tokenizer.model")
        dest_tokenizer_model = os.path.join(MERGED_MODEL_PATH, "tokenizer.model")
        if os.path.exists(source_tokenizer_model):
            shutil.copyfile(source_tokenizer_model, dest_tokenizer_model)
            print("âœ… tokenizer.model å¤åˆ¶å®Œæˆã€‚")
        else:
            print("âš ï¸ è­¦å‘Š: åœ¨æºç›®å½•ä¸­æœªæ‰¾åˆ° tokenizer.modelï¼Œè·³è¿‡å¤åˆ¶ã€‚")
    except Exception as e:
        print(f"âŒ å¤åˆ¶ tokenizer.model æ—¶å‡ºé”™: {e}")

    print(f"ğŸ‰ æˆåŠŸï¼åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜è‡³: {MERGED_MODEL_PATH}")
    print("=" * 30)


if __name__ == "__main__":
    merge_lora_with_base_model()
